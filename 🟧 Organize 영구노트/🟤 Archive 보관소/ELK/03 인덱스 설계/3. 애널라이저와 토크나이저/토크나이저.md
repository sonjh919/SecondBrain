#Elasticsearch 

## 토크나이저
토크나이저는 캐릭터 스트림을 받아서 여러 토큰으로 쪼개어 **토큰 스트림**을 만든다. [[애널라이저]]는 **한 개의 토크나이저만 지정**할 수 있다.

> [!summary]+ 
> 1. standard 토크나이저
> 2. keyword 토크나이저
> 3. ngram 토크나이저
> 4. edge_ngram 토크나이저

## standard 토크나이저
가장 기본적인 토크나이저다. Unicode Text Segmentation 알고리즘을 사용하여 **텍스트를 단어 단위로 나눈다.** 대부분의 문장 부호가 사라진다.

> [!check]+ 
> 필드 매핑에 특정 애널라이저를 지정하지 않으면 기본값으로 standard 애널라이저가 적용되는데, standard 애널라이저가 standard 토크나이저를 이용한다.

## keyword 토크나이저
들어온 텍스트를 쪼개지 않고 그대로 내보낸다. 즉 커다란 단일 토큰을 내보낸다.

```json
POST _analyze
{
	"tokenizer": "keyword",
	"text": "Hello, HELLO, World!"
}

//return
{
  "tokens": [
    {
      "token": "Hello, HELLO, World!",
      "start_offset": 0,
      "end_offset": 20,
      "type": "word",
      "position": 0
    }
  ]
}
```

> [!tip]+ 
> 토큰나이저에서 특별한 동작을 수행하지 않지만, 여러 [[캐릭터 필터]], [[토큰 필터]]와 함께 조합하면 다양한 커스텀 지정이 가능하다.
## ngram 토크나이저
**텍스트를 `min_gram` 값 이상 `max_gram`값 이하의 단위로 쪼갠다.** 

> [!example]+ 
> 3과 4의 단위로 쪼개지는 모든 경우의 수를 보면, 총 21개의 토큰으로 쪼개진다.
```
POST _analyze
{
	"tokenizer": {
	  "type": "ngram",
	  "min_gram": 3,
	  "max_gram": 4
	  },
	"text": "Hello, World!"
}
```

하지만 `,` ` ` 등 활용 의미가 없는 토큰도 포함된다. 이런 문제를 피하기 위해 ngram 토크나이저는 **token_chars** 속성을 통해 토큰에 포함시킬 타입의 문자를 지정할 수 있다.

### token_chars
특별히 지정하지 않을 때의 기본 동작은 모든 문자를 허용한다.

| 종류          | 설명                                      |
| ----------- | --------------------------------------- |
| Letter      | 언어의 글자로 분류되는 문자                         |
| Digit       | 숫자로 분류되는 문자                             |
| Whitespace  | 띄어쓰기나 줄바꿈 문자 등 공백으로 인식되는 문자             |
| punctuation | !나 "등의 문장 부호                            |
| symbol      | $같은 기호                                  |
| custom      | custom_token_chars 설정을 통해 따로 지정한 커스텀 문자 |
> [!example]+ 
> 10 개의 토큰으로 쪼개진다. 공백 문자를 전후해 위치한 문자로 구성된 "oWo"같은 토큰은 포함되지 않는다.
> 
> Hel, Hell, ell, ello, llo, Wor, Worl, orl, orld, rld
```json
POST _analyze
{
	"tokenizer": {
	  "type": "ngram",
	  "min_gram": 3,
	  "max_gram": 4,
	  "token_chars": ["letter"]
	  },
	"text": "Hello, World!"
}
```

> [!tip]+ 
> ngram 토크나이저는 ES에서 RDB의 `LIKE *검색어*`와 유사한 검색을 구현하고 싶을 때, 자동 완성 관련 서비스를 구현하고 싶을 때 등에 주로 활용한다.

> [!caution]+ 
> `min_gram`과 `max_gram`의 차이가 2 이상으로 벌어진다면 분석 시도가 실패하게 된다. 이 제한값은 `index.max_ngram_diff` 인덱스 설정(인덱스 생성시 지정)을 통해 지정할 수 있으며 기본값은 1이다.


## edge_ngram
edge_ngram 토크나이저는 ngram 토크나이저와 유사한 동작을 수행한다.

> [!note]+ 차이점
> 하지만 ngram과 다르게 **생성된 모든 토큰의 시작 글자를 단어의 시작 글자로 고정시켜서 생성**한다.

> [!example]+ 
> Hel, Hell, Wor, Worl의 총 4개 토큰으로 쪼개진다. 토큰의 시작은 단어의 시작 글자여야 하므로 ello같은 토큰은 포함되지 않는다.
```json
POST _analyze
{
	"tokenizer": {
	  "type": "edge_ngram",
	  "min_gram": 3,
	  "max_gram": 4,
	  "token_chars": ["letter"]
	  },
	"text": "Hello, World!"
}
```

## 그 외
> [!summary]+ 
> + **letter 토크나이저** : 공백, 특수문자 등 언어의 글자로 분류되는 문자가 아닌 문자를 만났을 때 쪼갠다.
> + **whitespace 토크나이저** : 공백 문자를 만났을 때 쪼갠다.
> + **pattern 토크나이저** : 지정한 정규표현식을 단어의 구분자로 사용하여 쪼갠다.